{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\trinh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\trinh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Category  5572 non-null   object\n",
      " 1   Message   5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('2cls_spam_text_cls.csv')\n",
    "df.info()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = df['Message'].to_numpy()\n",
    "categories = df['Category'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Create translate table\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # Apply the translation\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def remove_stopword(tokens):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "\n",
    "def stemming(tokens):\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = lowercase(text)\n",
    "    text = remove_punctuation(text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = remove_stopword(tokens)\n",
    "    tokens = stemming(tokens)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the preprocess steps to the messages data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [preprocess_text(message) for message in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'jurong', 'point', 'crazi', 'avail', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amor', 'wat']\n"
     ]
    }
   ],
   "source": [
    "print(messages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Message --preprocess--> a list of words <br>\n",
    "Scan though all the lists and build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(messages):\n",
    "    vocabulary = []\n",
    "\n",
    "    for message in messages:\n",
    "        for word in message:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary.append(word)\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8190\n"
     ]
    }
   ],
   "source": [
    "vocabulary = build_vocabulary(messages)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create features (bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(tokens, vocabulary):\n",
    "    features = np.zeros(len(vocabulary))\n",
    "\n",
    "    for word in tokens:\n",
    "        if word in vocabulary:\n",
    "            features[vocabulary.index(word)] += 1\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5572\n"
     ]
    }
   ],
   "source": [
    "x = [create_features(message, vocabulary) for message in messages]\n",
    "x = np.array(x)\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding \n",
    "Ham -> 0 <br>\n",
    "Spam ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['ham' 'spam']\n",
      "Labels: [0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(categories)\n",
    "print(f'Classes: {le.classes_}')\n",
    "print(f'Labels: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, test, validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = 0.2\n",
    "TEST_SIZE = 0.125\n",
    "SEED = 0\n",
    "IS_SHUFFLE = True\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x, y,\n",
    "    test_size=VAL_SIZE,\n",
    "    shuffle=IS_SHUFFLE, \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_train, y_train,\n",
    "    test_size=TEST_SIZE,\n",
    "    shuffle=IS_SHUFFLE,\n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 3899\n",
      "Number of validation samples: 1115\n",
      "Number of test samples: 558\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training samples: {len(x_train)}')\n",
    "print(f'Number of validation samples: {len(x_val)}')\n",
    "print(f'Number of test samples: {len(x_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "model = model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 0.8816143497757848\n",
      "Test accuracy: 0.8620071684587813\n"
     ]
    }
   ],
   "source": [
    "y_val_pred = model.predict(x_val)\n",
    "y_test_pred = model.predict(x_test)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Val accuracy: {val_accuracy}')\n",
    "print(f'Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict a user's input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, vocabulary, le):\n",
    "    tokens = preprocess_text(text)\n",
    "    x = create_features(tokens, vocabulary)\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    y = model.predict(x)\n",
    "    prediction = le.inverse_transform(y)[0]\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n",
      "spam\n"
     ]
    }
   ],
   "source": [
    "test_input = \"Do you free on Sunday?\"\n",
    "prediction = predict(test_input, model, vocabulary, le) \n",
    "print(prediction)\n",
    "\n",
    "test_input = \"This is our new product\"\n",
    "prediction = predict(test_input, model, vocabulary, le) \n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 2: Dont use library model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create probability table so we can pluck the P(w|c) into Bayes formula, and the P(c) too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(-1, 1)\n",
    "xy = np.hstack((x, y))\n",
    "\n",
    "x_ham = np.array([row[:-1] for row in xy if row[-1] == 0])\n",
    "x_spam = np.array([row[:-1] for row in xy if row[-1] == 1])\n",
    "\n",
    "# Create ham vocabulary probability table\n",
    "ham_vocabulary_probability = np.sum(x_ham, axis=0)\n",
    "ham_vocabulary_probability += 1\n",
    "total_ham_words = x_ham.sum()\n",
    "ham_vocabulary_probability /= (total_ham_words + len(x_ham))\n",
    "\n",
    "# Create spam vocabulary probability table\n",
    "spam_vocabulary_probability = np.sum(x_spam, axis=0)\n",
    "spam_vocabulary_probability += 1\n",
    "total_spam_words = x_spam.sum()\n",
    "spam_vocabulary_probability /= (total_spam_words + len(x_spam))\n",
    "\n",
    "# Find the spam and ham probability\n",
    "spam_prob = len(x_ham) / len(x)\n",
    "ham_prob = len(x_spam) / len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use the Bayes formula to see which is bigger P(ham|w1, w2, ...) or P(spam|w1, w2, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Use log space to prevent underflow\n",
    "def predict_text(text):\n",
    "    tokens = preprocess_text(text)\n",
    "\n",
    "    # HAM\n",
    "    ham_pwc_part = 0\n",
    "    for token in tokens:\n",
    "        ham_pwc_part += np.log(ham_vocabulary_probability[vocabulary.index(token)])\n",
    "    full_ham = ham_pwc_part + np.log(ham_prob)\n",
    "\n",
    "    # SPAM\n",
    "    spam_pwc_part = 0\n",
    "    for token in tokens:\n",
    "        spam_pwc_part += np.log(spam_vocabulary_probability[vocabulary.index(token)])\n",
    "    full_spam = spam_pwc_part + np.log(ham_prob)\n",
    "\n",
    "    if full_ham >= full_spam:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "text = \"Nah I don't think he goes to usf, he lives around here though\"\n",
    "print(predict_text(text))\n",
    "text = \"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\"\n",
    "print(predict_text(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIO310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
